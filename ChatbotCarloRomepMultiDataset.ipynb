{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ChatbotCarloRomepMultiDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MAFktDt-GA_k",
        "Bxiz7_KRGdub",
        "QnzZ59s-RnRN"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarloRomeoGitHub/ML-Projects/blob/master/ChatbotCarloRomepMultiDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IucGDSeYqMpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1698bf5d-7fd7-4499-f781-57c823e2d854"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAFktDt-GA_k"
      },
      "source": [
        "\r\n",
        "\r\n",
        "## Import\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_kxfdP4hJUPB",
        "outputId": "0c55c8b4-17f2-451d-d449-b27ee29d90e6"
      },
      "source": [
        "!python --version\r\n",
        "!pip install tensorflow-addons==0.11.2\r\n",
        "!pip install tensorflow==2.3.0\r\n",
        "!pip install contractions\r\n",
        "!pip install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n",
            "Collecting tensorflow-addons==0.11.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/f8/d6fca180c123f2851035c4493690662ebdad0849a9059d56035434bff5c9/tensorflow_addons-0.11.2-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Successfully installed tensorflow-addons-0.11.2\n",
            "Collecting tensorflow==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/ae/0b08f53498417914f2274cc3b5576d2b83179b0cbb209457d0fde0152174/tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.3.3)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (2.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.36.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.0) (51.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.17.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, numpy, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/5f/91102df95715fdda07f56a7eba2baae983e2ae16a080eb52d79e08ec6259/contractions-0.0.45-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 24.6MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/92/b3c70b8cf2b76f7e3e8b7243d6f06f7cb3bab6ada237b1bce57604c5c519/pyahocorasick-1.4.1.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 58.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.1-cp36-cp36m-linux_x86_64.whl size=84334 sha256=8b3617fcd73e490ac1c2d5e4fc6d33e168d30a7b234c0a26618bf793c2758faf\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/ab/f7/cb39270df8f6126f3dd4c33d302357167086db460968cfc80c\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.2 contractions-0.0.45 pyahocorasick-1.4.1 textsearch-0.0.17\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pFnEemkzJDc"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip -O glove.zip\r\n",
        "# !unzip glove.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeY4tF7mGH07"
      },
      "source": [
        "# !wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip -O cornell_dataset.zip\r\n",
        "# !unzip cornell_dataset.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RebXH-JGWaB"
      },
      "source": [
        "\r\n",
        "\r\n",
        "> Import \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZNFU5nadLXw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from keras.layers import *\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional, StackedRNNCells, Dropout, Embedding, Attention, LSTM\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import contractions\n",
        "import tensorflow_addons as tfa\n",
        "import keras\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGxMy9X-GBSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16731f9a-af27-45e5-bd5b-67da889b7b35"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "len(stop)\n",
        "\n",
        "lemma = WordNetLemmatizer()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxiz7_KRGdub"
      },
      "source": [
        "\r\n",
        "\r\n",
        "## Preprocessing\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6G2oz7xGBSk"
      },
      "source": [
        "keep_list = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom',\n",
        "             'be',  'have', 'do', 'can', 'not', 'will', 'go', 'no', 'to', 'is', 'am', 'are', 'has',\n",
        "             'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
        "             'i', 'you', 'he', 'she', 'it', 'we', 'they']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW_xDV8uDqQn"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def remove_contractions(q):\r\n",
        "  questions_exp = []\r\n",
        "  answers_exp = []\r\n",
        "\r\n",
        "  for line in q:\r\n",
        "    expanded_words = []\r\n",
        "    t = line.split()\r\n",
        "    for word in t:\r\n",
        "      expanded_words.append(contractions.fix(word))\r\n",
        "    sent = ' '.join(expanded_words)\r\n",
        "    questions_exp.append(sent)\r\n",
        "\r\n",
        "  return questions_exp\r\n",
        "\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SD49Ym4EgO1"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def remove_punct(q):\r\n",
        "  q_p = []\r\n",
        "\r\n",
        "  for line in q:\r\n",
        "    txt = line.lower()\r\n",
        "    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\r\n",
        "    txt = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", txt)\r\n",
        "\r\n",
        "    q_p.append(txt)\r\n",
        "  return q_p\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PULQf9_wIKNX"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def lemm(q):\r\n",
        "\r\n",
        "  questions_lemm = []\r\n",
        " \r\n",
        "\r\n",
        "  for i in q:\r\n",
        "    token_list = i.split()\r\n",
        "    sent = ' '.join([lemma.lemmatize(token) for token in token_list])\r\n",
        "    questions_lemm.append(sent)\r\n",
        "\r\n",
        "  return questions_lemm\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnId5qhbbeCp"
      },
      "source": [
        "# q_len_char = []\r\n",
        "# for line in q_lemm:\r\n",
        "#   q_len_char.append(len(line))\r\n",
        "\r\n",
        "# a_len_char = []\r\n",
        "# for line in a_lemm:\r\n",
        "#   a_len_char.append(len(line))\r\n",
        "\r\n",
        "# q_len_word = []\r\n",
        "\r\n",
        "# for line in q_lemm:\r\n",
        "#   t = line.split()\r\n",
        "#   q_len_word.append(len(t))\r\n",
        "\r\n",
        "# a_len_word = []\r\n",
        "\r\n",
        "# for line in a_lemm:\r\n",
        "#   t = line.split()\r\n",
        "#   a_len_word.append(len(t))\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZon5UKoc6P7"
      },
      "source": [
        "# q_char_mean = 0\r\n",
        "# for x in q_len_char:\r\n",
        "#   q_char_mean += x\r\n",
        "# q_char_mean = q_char_mean/len(q_len_char)\r\n",
        "# q_char_mean"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NElFDxDidYqE"
      },
      "source": [
        "# q_word_mean = 0\r\n",
        "# for x in q_len_word:\r\n",
        "#   q_word_mean += x\r\n",
        "# q_word_mean = q_word_mean/len(q_len_word)\r\n",
        "# q_word_mean"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9W6HClOeAoL"
      },
      "source": [
        "# a_char_mean = 0\r\n",
        "# for x in a_len_char:\r\n",
        "#   a_char_mean += x\r\n",
        "# a_char_mean = a_char_mean/len(a_len_char)\r\n",
        "# a_char_mean"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b77qu7a7eAoN"
      },
      "source": [
        "# a_word_mean = 0\r\n",
        "# for x in a_len_word:\r\n",
        "#   a_word_mean += x\r\n",
        "# a_word_mean = a_word_mean/len(a_len_word)\r\n",
        "# a_word_mean"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlGZuPwIbvt7"
      },
      "source": [
        "# print('mean char q: ', q_char_mean)\r\n",
        "# print('mean word q: ', q_word_mean)\r\n",
        "# print('mean char a: ', a_char_mean)\r\n",
        "# print('mean word q: ', a_word_mean)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpslQeTxIzJ8"
      },
      "source": [
        "\r\n",
        "def create_voc(voc, q, a):\r\n",
        "  for question in q:\r\n",
        "    for word in question.split():\r\n",
        "        if word not in voc:\r\n",
        "            voc[word] = 1\r\n",
        "        else:\r\n",
        "            voc[word] += 1\r\n",
        "\r\n",
        "  for question in a:\r\n",
        "      for word in question.split():\r\n",
        "          if word not in voc:\r\n",
        "              voc[word] = 1\r\n",
        "          else:\r\n",
        "              voc[word] += 1\r\n",
        "  \r\n",
        "  print(\"Size of total vocab:\", len(voc))\r\n",
        "  return voc\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr5_qsOAJtZ3"
      },
      "source": [
        "\r\n",
        "def create_freq_voc(v_q, e_dct, t):\r\n",
        "  word_num = 0\r\n",
        "  for word, count in v_q.items():\r\n",
        "    if count >= t:\r\n",
        "      if word not in e_dct:\r\n",
        "        e_dct[word] = word_num\r\n",
        "        # dec_dict[word_num] = word\r\n",
        "        word_num += 1\r\n",
        "  \r\n",
        "  return e_dct\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ARZU_HnLb2t"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def remove_empty(q, a):\r\n",
        "  for i in q:\r\n",
        "    if i == ' ':\r\n",
        "      del q[q.index(i)]\r\n",
        "      del a[q.index(i)]\r\n",
        "\r\n",
        "  for i in a:\r\n",
        "    if i == ' ':\r\n",
        "      del a[a.index(i)]\r\n",
        "      del q[a.index(i)]\r\n",
        "\r\n",
        "  return q, a\r\n",
        "\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaIj2eVrdOEA"
      },
      "source": [
        "def remove_bad(q, a, bad):\r\n",
        "  questions_f = []\r\n",
        "  answers_f = []\r\n",
        "\r\n",
        "  for line in q:\r\n",
        "    t = line.split()\r\n",
        "    sent = []\r\n",
        "    for word in t:\r\n",
        "      if word not in bad:\r\n",
        "        sent.append(word)\r\n",
        "    questions_f.append(' '.join([y for y in sent]))\r\n",
        "\r\n",
        "  for line in a:\r\n",
        "    t = line.split()\r\n",
        "    sent = []\r\n",
        "    for word in t:\r\n",
        "      if word not in bad:\r\n",
        "        sent.append(word)\r\n",
        "    answers_f.append(' '.join([y for y in sent]))\r\n",
        "  return questions_f, answers_f\r\n",
        "\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROcux_Kf2gwi"
      },
      "source": [
        "def preprocessing(q, a, keep, rang, examples, threshold):\r\n",
        "  # SLICING #\r\n",
        "  q = q[:examples]\r\n",
        "  a = a[:examples]\r\n",
        "  # CONTRACTIONS #\r\n",
        "  q_exp = []\r\n",
        "  a_exp = []\r\n",
        "  q_exp = remove_contractions(q)\r\n",
        "  a_exp = remove_contractions(a)\r\n",
        "  # PUNCTUATION #\r\n",
        "  q_punct = []\r\n",
        "  a_punct = []\r\n",
        "  q_punct = remove_punct(q_exp)\r\n",
        "  a_punct = remove_punct(a_exp)\r\n",
        "  # LEMMATIZING #\r\n",
        "  q_lemm = []\r\n",
        "  a_lemm = []\r\n",
        "  q_lemm = lemm(q_punct)\r\n",
        "  a_lemm = lemm(a_punct)\r\n",
        "  # LIMIT RANGE OF WORDS #\r\n",
        "  q_range = []\r\n",
        "  for i in range(len(q_lemm)):\r\n",
        "    t = q_lemm[i].split()\r\n",
        "    q_range.append(' '.join(t[:(rang+5)]))\r\n",
        "  a_range = []\r\n",
        "  for i in range(len(a_lemm)):\r\n",
        "    t = a_lemm[i].split()\r\n",
        "    a_range.append(' '.join(t[:(rang+5)]))\r\n",
        "  # CREATE FREQUENCY DICTIONARY #\r\n",
        "  voc = {}\r\n",
        "  voc = create_voc(voc, q_lemm, a_lemm)\r\n",
        "  print(len(voc))\r\n",
        "  # CREATE DICTIONARY WITH THRESHOLD #\r\n",
        "  enc_dict = {}\r\n",
        "  enc_dict = create_freq_voc(voc, enc_dict, threshold)\r\n",
        "  print(len(enc_dict))\r\n",
        "  # REMOVE EMPTY ROWS #\r\n",
        "  q_empty = []\r\n",
        "  a_empty = []\r\n",
        "  q_empty, a_empty = remove_empty(q_range, a_range)\r\n",
        "  # CREATE BAD_LIST WORDS #\r\n",
        "  to_remove = []\r\n",
        "  for wrd in voc.keys():\r\n",
        "    if wrd not in keep:\r\n",
        "      if wrd not in enc_dict:\r\n",
        "        to_remove.append(wrd)\r\n",
        "  # REMOVE BAD WORDS FROM EVERY UTTERANCE\r\n",
        "  q_bad, a_bad = remove_bad(q_empty, a_empty, to_remove)\r\n",
        "  q_final = []\r\n",
        "  for i in range(len(q_bad)):\r\n",
        "    t = q_bad[i].split()\r\n",
        "    q_final.append(' '.join(t[:rang]))\r\n",
        "  a_final = []\r\n",
        "  for i in range(len(a_bad)):\r\n",
        "    t = a_bad[i].split()\r\n",
        "    a_final.append(' '.join(t[:rang]))\r\n",
        "  # ADDING START AND END TAG\r\n",
        "  for i in range(len(a_final)):\r\n",
        "    a_final[i] = '<start> ' + a_final[i] + ' <end>'\r\n",
        "    q_final[i] = '<start> ' + q_final[i] + ' <end>'\r\n",
        "\r\n",
        "  return q_final, a_final\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnzZ59s-RnRN"
      },
      "source": [
        "##Dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibo0NtbnS-Dm",
        "outputId": "3665606b-4673-4c61-a0a8-03e45c037b61"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Dati/archive.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Dati/archive.zip\n",
            "  inflating: Ant-Man.And.The.Wasp.txt  \n",
            "  inflating: Ant-Man.txt             \n",
            "  inflating: Avengers.Age.of.Ultron.txt  \n",
            "  inflating: Avengers.Endgame.txt    \n",
            "  inflating: Avengers.Infinity.War.txt  \n",
            "  inflating: Avengers.txt            \n",
            "  inflating: Black.Panther.txt       \n",
            "  inflating: Captain.America.Civil.War.txt  \n",
            "  inflating: Captain.America.The.First.Avenger.txt  \n",
            "  inflating: Captain.America.The.Winter.Soldier.txt  \n",
            "  inflating: Captain.Marvel.txt      \n",
            "  inflating: Doctor.Strange.txt      \n",
            "  inflating: Guardians.of.the.Galaxy.Vol. 2.txt  \n",
            "  inflating: Guardians.of.the.Galaxy.txt  \n",
            "  inflating: Iron-Man.2.txt          \n",
            "  inflating: Iron-Man.3.txt          \n",
            "  inflating: Iron-Man.txt            \n",
            "  inflating: Spider-Man.Far.From.Home.txt  \n",
            "  inflating: Spider-Man.Homecoming.txt  \n",
            "  inflating: The.Incredible.Hulk.txt  \n",
            "  inflating: Thor.Ragnarok.txt       \n",
            "  inflating: Thor.The.Dark.World.txt  \n",
            "  inflating: Thor.txt                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfdYBCp5eVw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef715e1-e569-4519-fcd8-65b22b14ff06"
      },
      "source": [
        "### SCRIPT MARVEL ###\r\n",
        "\r\n",
        "\r\n",
        "m_q = []\r\n",
        "m_a = []\r\n",
        "\r\n",
        "for f in os.listdir('/content/'):\r\n",
        "  if f.endswith('.txt'):\r\n",
        "    data = open(f, encoding='utf-8', errors='ignore').read().split('\\n')\r\n",
        "    for i in range(len(data)):\r\n",
        "      if (i%2)==0:\r\n",
        "        m_q.append(data[i])\r\n",
        "      else:\r\n",
        "        m_a.append(data[i])\r\n",
        "\r\n",
        "l_q = len(m_q)\r\n",
        "l_a = len(m_a)\r\n",
        "\r\n",
        "print(len(m_q), len(m_a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18426 18415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Qpc0gi-MKP",
        "outputId": "80c17f61-73e5-4cd6-8249-40d190f7a257"
      },
      "source": [
        "mq_final, ma_final = preprocessing(m_q, m_a, keep_list, 5, 18000, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 10368\n",
            "10368\n",
            "454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs17SLB-FDJV",
        "outputId": "67ee7180-b844-4106-abc3-b620d359f963"
      },
      "source": [
        "len(mq_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9H6ktMZF0Ez"
      },
      "source": [
        "ubuntu = pd.read_csv('/content/drive/MyDrive/Dati/dialogueText.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxcU-XXGZDd"
      },
      "source": [
        "### SCRIPT UBUNTU ###\r\n",
        "u_q = []\r\n",
        "u_a = []\r\n",
        "for i in range(len(ubuntu['text'])):\r\n",
        "  if (i%2)==0:\r\n",
        "    u_q.append(str(ubuntu['text'][i]))\r\n",
        "  else:\r\n",
        "    u_a.append(str(ubuntu['text'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uX-Ep5hGrW3",
        "outputId": "2dcfba6d-3c50-4bf0-963b-f5d983a57f26"
      },
      "source": [
        "len(u_q), len(u_a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(519162, 519162)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAN6sILIGu3-",
        "outputId": "e2dd5c54-e2bf-4e36-e1ff-c0bfb14fb0b2"
      },
      "source": [
        "uq_final, ua_final = preprocessing(u_q, u_a, keep_list, 5, 74000, 250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 46849\n",
            "46849\n",
            "647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFuvMcyVIcjW"
      },
      "source": [
        "simpsons = pd.read_csv('/content/drive/MyDrive/Dati/simpsons_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30c71h-iIsHp"
      },
      "source": [
        "s_q = []\r\n",
        "s_a = []\r\n",
        "for i in range(len(simpsons['spoken_words'])):\r\n",
        "  if (i%2)==0:\r\n",
        "    s_q.append(str(simpsons['spoken_words'][i]))\r\n",
        "  else:\r\n",
        "    s_a.append(str(simpsons['spoken_words'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p2AS0fcI_OA",
        "outputId": "fe466503-97a7-4d65-b4b4-f818d43e89ca"
      },
      "source": [
        "len(s_q), len(s_a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79157, 79157)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vk6PwdRJC9r",
        "outputId": "d962cb2b-e2ac-4448-ca2d-22ec905a9505"
      },
      "source": [
        "sq_final, sa_final = preprocessing(s_q, s_a, keep_list, 5, 75000, 250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 38984\n",
            "38984\n",
            "495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_EWSfXN5hD_"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "lines = open('/content/drive/MyDrive/Dati/movie_lines.txt', encoding='utf-8',\r\n",
        "             errors='ignore').read().split('\\n')\r\n",
        "\r\n",
        "convers = open('/content/drive/MyDrive/Dati/movie_conversations.txt', encoding='utf-8',\r\n",
        "errors='ignore').read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypQrTgQW2At"
      },
      "source": [
        "\r\n",
        "exchn = []\r\n",
        "for conver in convers:\r\n",
        "    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\r\n",
        "\r\n",
        "diag = {}\r\n",
        "for line in lines:\r\n",
        "    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\r\n",
        "\r\n",
        "## delete\r\n",
        "del(lines, convers, conver, line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBO67_IRG0i"
      },
      "source": [
        "c_q = []\r\n",
        "c_a = []\r\n",
        "for conver in exchn:\r\n",
        "    for i in range(len(conver) - 1):\r\n",
        "      if (i%2)==0:\r\n",
        "        c_q.append(diag[conver[i]])\r\n",
        "      else:\r\n",
        "        c_a.append(diag[conver[i]])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDtBhN2fNmwn",
        "outputId": "666c6851-11f5-419c-df1a-b21024eb48a7"
      },
      "source": [
        "len(c_q), len(c_a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(138135, 83481)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un9b186kNjaJ",
        "outputId": "8f27edaf-72ed-451e-c8ca-1ea122fd14b4"
      },
      "source": [
        "cq_final, ca_final = preprocessing(c_q, c_a, keep_list, 5, 83000, 250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 39929\n",
            "39929\n",
            "561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT6Ycd4IPHSa",
        "outputId": "0fe52ddc-3e3e-44df-d443-3c637fb2a75e"
      },
      "source": [
        "len(cq_final), len(ca_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83000, 83000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rbaiCYUKtT8"
      },
      "source": [
        "q_final = []\r\n",
        "a_final = []\r\n",
        "q_final = mq_final + sq_final + uq_final + cq_final\r\n",
        "a_final = ma_final + sa_final + ua_final + ca_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGkq7qLutEN6"
      },
      "source": [
        "f=open('chat_qMulti.txt','w')\r\n",
        "for ele in q_final:\r\n",
        "    f.write(ele+'\\n')\r\n",
        "\r\n",
        "f.close()\r\n",
        "\r\n",
        "f=open('chat_aMulti.txt','w')\r\n",
        "for ele in a_final:\r\n",
        "    f.write(ele+'\\n')\r\n",
        "\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN2yftR9VDAY"
      },
      "source": [
        "##Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-_0JbSbynS_"
      },
      "source": [
        "q_final = open('/content/drive/MyDrive/chat_qMulti5.txt', encoding='utf-8',\r\n",
        "              errors='ignore').read().split('\\n')\r\n",
        "\r\n",
        "a_final = open('/content/drive/MyDrive/chat_aMulti5.txt', encoding='utf-8',\r\n",
        "              errors='ignore').read().split('\\n')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsbL7rppLP52",
        "outputId": "68350b85-b1be-4ff2-b246-ebf10f49e4fb"
      },
      "source": [
        "len(q_final), len(a_final)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250001, 250001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4yUwDjDlQXI"
      },
      "source": [
        "num_examples = 250000\r\n",
        "q_final = q_final[:num_examples]\r\n",
        "a_final = a_final[:num_examples]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_9ztTUuOU4h",
        "outputId": "42a49023-33d8-4d5e-bf1c-2751baf17b34"
      },
      "source": [
        "for i in range(5):\r\n",
        "  print (q_final[i] + '\\t' + a_final[i])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> on your left <end>\t<start> on your left <end>\n",
            "<start> on my left got it <end>\t<start> do not say it do <end>\n",
            "<start> on your left come on <end>\t<start> need a <end>\n",
            "<start> i need a new of <end>\t<start> dude you just like in <end>\n",
            "<start> i guess i got a <end>\t<start> really you should be of <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HObdAi6Vp5rO"
      },
      "source": [
        "max_q = max(len(x) for x in q_final)\r\n",
        "max_a = max(len(x) for x in a_final)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bw6NgqCrra4",
        "outputId": "006e0d5e-f268-4a88-fce9-55f8d6c2f610"
      },
      "source": [
        "max_len = 0\r\n",
        "if max_q > max_a:\r\n",
        "  max_len = max_q\r\n",
        "else: max_len = max_a\r\n",
        "max_len"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIV_mcOFFPvA"
      },
      "source": [
        "def tokenize(lang):\r\n",
        "        # lang = list of sentences in a language\r\n",
        "        \r\n",
        "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\r\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\r\n",
        "  lang_tokenizer.fit_on_texts(lang)\r\n",
        "\r\n",
        "  ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \r\n",
        "  ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\r\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang) \r\n",
        "\r\n",
        "  ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \r\n",
        "  ## and pads the sequences to match the longest sequences in the given input\r\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\r\n",
        "\r\n",
        "  return tensor, lang_tokenizer\r\n",
        "\r\n",
        "BUFFER_SIZE = 45000\r\n",
        "BATCH_SIZE = 9000\r\n",
        "# Let's limit the #training examples for faster training\r\n",
        "\r\n",
        "input_tensor, inp_lang = tokenize(q_final)\r\n",
        "target_tensor, targ_lang = tokenize(a_final)\r\n",
        "\r\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\r\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "\r\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\r\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "\r\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2lCTy4vKOkB",
        "outputId": "1a31affc-8e2d-472f-f0a9-0523a6232f8e"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([9000, 7]), TensorShape([9000, 7]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "source": [
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "max_length_input = example_input_batch.shape[1]\n",
        "max_length_output = example_target_batch.shape[1]\n",
        "\n",
        "units = 512\n",
        "steps_per_epoch = num_examples//BATCH_SIZE\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-yY9c6aIu1h",
        "outputId": "2592c158-4ed7-4022-e273-a2f085fa0522"
      },
      "source": [
        "print(\"max_length_input, max_length_target, vocab_size_input, vocab_size_target\")\n",
        "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_length_input, max_length_target, vocab_size_input, vocab_size_target\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 7, 1028, 1028)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVTvmnRIh4MD",
        "outputId": "9a069f37-dc16-4c7c-d19b-3de85aa2cebb"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "print(\"Glove Loded!\")\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Loded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUbRBznMeKUn"
      },
      "source": [
        "embedding_dimention = 50\r\n",
        "\r\n",
        "embedding_matrix_enc = np.zeros((len(inp_lang.word_index)+1, embedding_dimention))\r\n",
        "for word, i in inp_lang.word_index.items():\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "      # words not found in embedding index will be all-zeros.\r\n",
        "        embedding_matrix_enc[i] = embedding_vector\r\n",
        "embed_enc = Embedding(len(inp_lang.word_index)+1, \r\n",
        "              50, \r\n",
        "              input_length=max_length_input,\r\n",
        "              trainable=True)\r\n",
        "\r\n",
        "embed_enc.build((None,))\r\n",
        "embed_enc.set_weights([embedding_matrix_enc])\r\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJLI-KwQe8xz"
      },
      "source": [
        "embedding_matrix_dec = np.zeros((len(targ_lang.word_index)+1, embedding_dimention))\r\n",
        "for word, i in targ_lang.word_index.items():\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "      # words not found in embedding index will be all-zeros.\r\n",
        "        embedding_matrix_dec[i] = embedding_vector\r\n",
        "embed_dec = Embedding(len(targ_lang.word_index)+1, \r\n",
        "              50, \r\n",
        "              \r\n",
        "              input_length=max_length_output,\r\n",
        "              trainable=True)\r\n",
        "\r\n",
        "embed_dec.build((None,))\r\n",
        "embed_dec.set_weights([embedding_matrix_dec])\r\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "##### \n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_layer, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    # self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.embedding = embedding_layer\n",
        "    ##-------- LSTM layer in Encoder ------- ##\n",
        "    self.lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform'\n",
        "                                   ))\n",
        "    \n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, fh, fc, bh, bc = self.lstm_layer(x, initial_state = hidden)\n",
        "\n",
        "    h = Concatenate()([fh, bh])\n",
        "    c = Concatenate()([fc, bc])\n",
        "    return output, h, c\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)) for i in range(4)] "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gSVh05Jl6l",
        "outputId": "3daf36f9-d76c-451f-98d9-0dd7a7892e01"
      },
      "source": [
        "## Test Encoder Stack\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embed_enc, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (9000, 7, 1024)\n",
            "Encoder h vecotr shape: (batch size, units) (9000, 1024)\n",
            "Encoder c vector shape: (batch size, units) (9000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_layer, dec_units, batch_sz, attention_type='luong'):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "    \n",
        "    # Embedding Layer\n",
        "    # self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.embedding = embedding_layer\n",
        "    \n",
        "    #Final Dense layer on which softmax will be applied\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "   \n",
        "\n",
        "\n",
        "    # Sampler\n",
        "    # self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    self.sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(sampling_probability=0.1, embedding_fn=self.embedding) ###\n",
        "\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
        "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
        "\n",
        "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "    \n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def get_attention_weights(self):\n",
        "    w = self.rnn_cell.weights\n",
        "    return w\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
        "    \n",
        "    return outputs\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaiO0Z6_Ml1c",
        "outputId": "fcaf8920-110a-4af8-8e3d-d46ae3078a98"
      },
      "source": [
        "# Test decoder stack\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embed_dec, units*2, BATCH_SIZE, 'luong')\n",
        "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "decoder.attention_mechanism.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
        "\n",
        "\n",
        "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
        "\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder Outputs Shape:  (9000, 6, 1028)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuPv_jCPVMb1"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss  "
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmC3hRycTtLZ"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def accuracy_function(real, pred):\r\n",
        "  y_pred = pred\r\n",
        "  y_true = real\r\n",
        "  pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\r\n",
        "  correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\r\n",
        "\r\n",
        "  mask = K.cast(K.greater(y_true, 0), dtype='float32')\r\n",
        "  n_correct = K.sum(mask * correct)\r\n",
        "  n_total = K.sum(mask)\r\n",
        "  accuracy = n_correct/n_total\r\n",
        "  accuracy = tf.reduce_mean(accuracy)\r\n",
        "  return accuracy"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkKIIKFZa-Fq",
        "outputId": "c9d07c6a-448c-41cb-8965-06cc72c655fb"
      },
      "source": [
        "!rm -r /content/training_checkpoints/"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/training_checkpoints/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg1aLKI5hnd5"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmD70Q-zTUmI"
      },
      "source": [
        "# # restoring the latest checkpoint in checkpoint_dir\r\n",
        "# checkpoint.restore(tf.train.latest_checkpoint('/content/drive/MyDrive/training_checkpoints768Units100Epochs1Run/'))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    pred= decoder(dec_input, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real, logits)\n",
        "    accuracy = accuracy_function(real, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss, accuracy"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shORtigpAhXp"
      },
      "source": [
        "@tf.function\r\n",
        "def val_step(inp, targ, enc_hidden):\r\n",
        "  loss = 0\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\r\n",
        "\r\n",
        "\r\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\r\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\r\n",
        "\r\n",
        "    # Set the AttentionMechanism object with encoder_outputs\r\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\r\n",
        "\r\n",
        "    # Create AttentionWrapperState as initial_state for decoder\r\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\r\n",
        "    pred= decoder(dec_input, decoder_initial_state)\r\n",
        "    logits = pred.rnn_output\r\n",
        "    loss = loss_function(real, logits)\r\n",
        "    accuracy = accuracy_function(real, logits)\r\n",
        "\r\n",
        "  # variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  # gradients = tape.gradient(loss, variables)\r\n",
        "  # optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "\r\n",
        "  return loss, accuracy"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H5j98oboMfI"
      },
      "source": [
        "run = 0"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7B1JBiUdMI"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egKtsB7nR8Pu"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/256U50E3R.zip"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYoh3PrQR4Hw"
      },
      "source": [
        "# # restoring the latest checkpoint in checkpoint_dir\n",
        "# checkpoint.restore(tf.train.latest_checkpoint('/content/content/256U50E3R/training_checkpoints256Units50Epochs3Run/'))\n",
        "# # restoring the specific checkpoint in checkpoint_dir\n",
        "# # checkpoint.restore(tf.train.latest_checkpoint('/content/training_checkpoints512Units100Epochs1Run')).expect_partial()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87d0819-8896-4481-feea-17b33c9a2627"
      },
      "source": [
        "EPOCHS = 100\n",
        "train_loss_vec = []\n",
        "train_accuracy_vec = []\n",
        "val_loss_vec = []\n",
        "val_accuracy_vec = []\n",
        "time_vec = []\n",
        "run += 1\n",
        "print('---TRAINING PHASE---\\n')\n",
        "print('MODEL: UNITS {} EPOCHS {} RUN {}'.format(units, EPOCHS, run))\n",
        "start_training = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_train_loss = 0\n",
        "  total_train_accuracy = 0\n",
        "  total_val_loss = 0\n",
        "  total_val_accuracy = 0\n",
        "  \n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    train_loss, train_accuracy = train_step(inp, targ, enc_hidden)\n",
        "    total_train_loss += train_loss\n",
        "    total_train_accuracy += train_accuracy\n",
        "\n",
        "    # if batch % 100 == 0:\n",
        "    #   print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "    #                                                batch,\n",
        "    #                                                batch_loss.numpy(),\n",
        "    #                                                batch_accuracy.numpy()))\n",
        "  train_loss_vec.append('{:.4f}'.format(total_train_loss / steps_per_epoch))\n",
        "  train_accuracy_vec.append('{:.4f}'.format(total_train_accuracy / steps_per_epoch))\n",
        "    \n",
        "  for (batch, (inp, targ)) in enumerate(val_dataset.take(steps_per_epoch)):\n",
        "    val_loss, val_accuracy = val_step(inp, targ, enc_hidden)\n",
        "    total_val_loss += val_loss\n",
        "    total_val_accuracy += val_accuracy\n",
        "\n",
        "    \n",
        "\n",
        "    # if batch % 100 == 0:\n",
        "    #   print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "    #                                                val_loss.numpy(),\n",
        "    #                                                val_accuracy.numpy()))\n",
        "      # loss_vec.append('{:.4f}'.format(batch_loss.numpy()))\n",
        "      # accuracy_vec.append('{:.4f}'.format(batch_accuracy.numpy()))\n",
        "\n",
        "  val_loss_vec.append('{:.4f}'.format(total_val_loss / steps_per_epoch))\n",
        "  val_accuracy_vec.append('{:.4f}'.format(total_val_accuracy / steps_per_epoch))\n",
        "\n",
        "\n",
        "  \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 20 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch: {} | Loss: {:.4f} | Accuracy: {:.4f} | Val Loss: {:.4f} | Val Accuracy: {:.4f}'.format(epoch + 1,\n",
        "                                      total_train_loss / steps_per_epoch,\n",
        "                                      total_train_accuracy / steps_per_epoch,\n",
        "                                      total_val_loss / steps_per_epoch,\n",
        "                                      total_val_accuracy / steps_per_epoch\n",
        "                                      ))\n",
        "  # print('Epoch {} Accuracy {:.4f}'.format(epoch + 1,\n",
        "  #                                     total_accuracy / steps_per_epoch))\n",
        "  # print('Epoch {} Val Loss {:.4f}'.format(epoch + 1,\n",
        "  #                                     total_val_loss / steps_per_epoch))\n",
        "  # print('Epoch {} Accuracy {:.4f}'.format(epoch + 1,\n",
        "  #                                     total_val_accuracy / steps_per_epoch))\n",
        "  epoch_time = time.time() - start\n",
        "  time_vec.append(epoch_time)\n",
        "  print('Time taken for epoch {} {} sec\\n'.format(epoch +1 , epoch_time))\n",
        "total_time = time.time() - start_training\n",
        "print('Time taken for the whole training phase is: {} sec'.format(total_time))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---TRAINING PHASE---\n",
            "\n",
            "MODEL: UNITS 512 EPOCHS 100 RUN 1\n",
            "Epoch: 1 | Loss: 3.9157 | Accuracy: 0.1775 | Val Loss: 0.2614 | Val Accuracy: 0.0156\n",
            "Time taken for epoch 1 51.65524697303772 sec\n",
            "\n",
            "Epoch: 2 | Loss: 3.1716 | Accuracy: 0.2051 | Val Loss: 0.2488 | Val Accuracy: 0.0169\n",
            "Time taken for epoch 2 43.67977285385132 sec\n",
            "\n",
            "Epoch: 3 | Loss: 3.0813 | Accuracy: 0.2147 | Val Loss: 0.2433 | Val Accuracy: 0.0176\n",
            "Time taken for epoch 3 43.914191007614136 sec\n",
            "\n",
            "Epoch: 4 | Loss: 3.0029 | Accuracy: 0.2273 | Val Loss: 0.2370 | Val Accuracy: 0.0188\n",
            "Time taken for epoch 4 43.999591588974 sec\n",
            "\n",
            "Epoch: 5 | Loss: 2.9219 | Accuracy: 0.2406 | Val Loss: 0.2308 | Val Accuracy: 0.0198\n",
            "Time taken for epoch 5 43.851826667785645 sec\n",
            "\n",
            "Epoch: 6 | Loss: 2.8513 | Accuracy: 0.2495 | Val Loss: 0.2263 | Val Accuracy: 0.0202\n",
            "Time taken for epoch 6 43.999948501586914 sec\n",
            "\n",
            "Epoch: 7 | Loss: 2.7974 | Accuracy: 0.2584 | Val Loss: 0.2224 | Val Accuracy: 0.0209\n",
            "Time taken for epoch 7 44.093581438064575 sec\n",
            "\n",
            "Epoch: 8 | Loss: 2.8741 | Accuracy: 0.2519 | Val Loss: 0.2506 | Val Accuracy: 0.0132\n",
            "Time taken for epoch 8 44.113158226013184 sec\n",
            "\n",
            "Epoch: 9 | Loss: 2.8753 | Accuracy: 0.2440 | Val Loss: 0.2224 | Val Accuracy: 0.0209\n",
            "Time taken for epoch 9 44.021119356155396 sec\n",
            "\n",
            "Epoch: 10 | Loss: 2.7463 | Accuracy: 0.2669 | Val Loss: 0.2191 | Val Accuracy: 0.0215\n",
            "Time taken for epoch 10 44.08615589141846 sec\n",
            "\n",
            "Epoch: 11 | Loss: 2.7134 | Accuracy: 0.2725 | Val Loss: 0.2172 | Val Accuracy: 0.0218\n",
            "Time taken for epoch 11 44.04543876647949 sec\n",
            "\n",
            "Epoch: 12 | Loss: 2.6945 | Accuracy: 0.2748 | Val Loss: 0.2159 | Val Accuracy: 0.0220\n",
            "Time taken for epoch 12 44.16236734390259 sec\n",
            "\n",
            "Epoch: 13 | Loss: 2.6787 | Accuracy: 0.2763 | Val Loss: 0.2149 | Val Accuracy: 0.0221\n",
            "Time taken for epoch 13 44.0592360496521 sec\n",
            "\n",
            "Epoch: 14 | Loss: 2.6646 | Accuracy: 0.2781 | Val Loss: 0.2141 | Val Accuracy: 0.0223\n",
            "Time taken for epoch 14 44.111886501312256 sec\n",
            "\n",
            "Epoch: 15 | Loss: 2.6533 | Accuracy: 0.2799 | Val Loss: 0.2130 | Val Accuracy: 0.0223\n",
            "Time taken for epoch 15 44.07824397087097 sec\n",
            "\n",
            "Epoch: 16 | Loss: 2.6403 | Accuracy: 0.2815 | Val Loss: 0.2126 | Val Accuracy: 0.0224\n",
            "Time taken for epoch 16 44.121702909469604 sec\n",
            "\n",
            "Epoch: 17 | Loss: 2.6314 | Accuracy: 0.2824 | Val Loss: 0.2118 | Val Accuracy: 0.0225\n",
            "Time taken for epoch 17 44.138686180114746 sec\n",
            "\n",
            "Epoch: 18 | Loss: 2.6227 | Accuracy: 0.2832 | Val Loss: 0.2112 | Val Accuracy: 0.0225\n",
            "Time taken for epoch 18 44.19797468185425 sec\n",
            "\n",
            "Epoch: 19 | Loss: 2.6127 | Accuracy: 0.2842 | Val Loss: 0.2109 | Val Accuracy: 0.0226\n",
            "Time taken for epoch 19 44.13322997093201 sec\n",
            "\n",
            "Epoch: 20 | Loss: 2.6044 | Accuracy: 0.2851 | Val Loss: 0.2101 | Val Accuracy: 0.0227\n",
            "Time taken for epoch 20 44.60334086418152 sec\n",
            "\n",
            "Epoch: 21 | Loss: 2.5965 | Accuracy: 0.2859 | Val Loss: 0.2096 | Val Accuracy: 0.0228\n",
            "Time taken for epoch 21 44.16948938369751 sec\n",
            "\n",
            "Epoch: 22 | Loss: 2.5892 | Accuracy: 0.2867 | Val Loss: 0.2093 | Val Accuracy: 0.0228\n",
            "Time taken for epoch 22 44.27661204338074 sec\n",
            "\n",
            "Epoch: 23 | Loss: 2.5815 | Accuracy: 0.2876 | Val Loss: 0.2089 | Val Accuracy: 0.0228\n",
            "Time taken for epoch 23 44.173134565353394 sec\n",
            "\n",
            "Epoch: 24 | Loss: 2.5737 | Accuracy: 0.2886 | Val Loss: 0.2085 | Val Accuracy: 0.0229\n",
            "Time taken for epoch 24 44.12670016288757 sec\n",
            "\n",
            "Epoch: 25 | Loss: 2.5662 | Accuracy: 0.2892 | Val Loss: 0.2083 | Val Accuracy: 0.0229\n",
            "Time taken for epoch 25 44.082963943481445 sec\n",
            "\n",
            "Epoch: 26 | Loss: 2.5609 | Accuracy: 0.2902 | Val Loss: 0.2081 | Val Accuracy: 0.0230\n",
            "Time taken for epoch 26 44.217039346694946 sec\n",
            "\n",
            "Epoch: 27 | Loss: 2.5545 | Accuracy: 0.2910 | Val Loss: 0.2078 | Val Accuracy: 0.0230\n",
            "Time taken for epoch 27 44.17956566810608 sec\n",
            "\n",
            "Epoch: 28 | Loss: 2.5474 | Accuracy: 0.2916 | Val Loss: 0.2076 | Val Accuracy: 0.0230\n",
            "Time taken for epoch 28 44.24581456184387 sec\n",
            "\n",
            "Epoch: 29 | Loss: 2.5408 | Accuracy: 0.2923 | Val Loss: 0.2075 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 29 44.12132787704468 sec\n",
            "\n",
            "Epoch: 30 | Loss: 2.5356 | Accuracy: 0.2927 | Val Loss: 0.2075 | Val Accuracy: 0.0230\n",
            "Time taken for epoch 30 44.1178195476532 sec\n",
            "\n",
            "Epoch: 31 | Loss: 2.5882 | Accuracy: 0.2836 | Val Loss: 0.2140 | Val Accuracy: 0.0220\n",
            "Time taken for epoch 31 44.293367862701416 sec\n",
            "\n",
            "Epoch: 32 | Loss: 2.5793 | Accuracy: 0.2856 | Val Loss: 0.2078 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 32 44.24954319000244 sec\n",
            "\n",
            "Epoch: 33 | Loss: 2.5359 | Accuracy: 0.2923 | Val Loss: 0.2072 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 33 44.15142607688904 sec\n",
            "\n",
            "Epoch: 34 | Loss: 2.5242 | Accuracy: 0.2939 | Val Loss: 0.2068 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 34 44.10337424278259 sec\n",
            "\n",
            "Epoch: 35 | Loss: 2.5149 | Accuracy: 0.2946 | Val Loss: 0.2069 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 35 44.175962686538696 sec\n",
            "\n",
            "Epoch: 36 | Loss: 2.5074 | Accuracy: 0.2954 | Val Loss: 0.2070 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 36 44.202969551086426 sec\n",
            "\n",
            "Epoch: 37 | Loss: 2.4996 | Accuracy: 0.2963 | Val Loss: 0.2068 | Val Accuracy: 0.0232\n",
            "Time taken for epoch 37 44.210835218429565 sec\n",
            "\n",
            "Epoch: 38 | Loss: 2.4941 | Accuracy: 0.2969 | Val Loss: 0.2067 | Val Accuracy: 0.0233\n",
            "Time taken for epoch 38 44.22455954551697 sec\n",
            "\n",
            "Epoch: 39 | Loss: 2.4884 | Accuracy: 0.2975 | Val Loss: 0.2070 | Val Accuracy: 0.0233\n",
            "Time taken for epoch 39 44.21939945220947 sec\n",
            "\n",
            "Epoch: 40 | Loss: 2.4791 | Accuracy: 0.2984 | Val Loss: 0.2070 | Val Accuracy: 0.0233\n",
            "Time taken for epoch 40 44.4990291595459 sec\n",
            "\n",
            "Epoch: 41 | Loss: 2.4706 | Accuracy: 0.2994 | Val Loss: 0.2074 | Val Accuracy: 0.0232\n",
            "Time taken for epoch 41 44.17366695404053 sec\n",
            "\n",
            "Epoch: 42 | Loss: 2.4633 | Accuracy: 0.3002 | Val Loss: 0.2077 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 42 44.249009132385254 sec\n",
            "\n",
            "Epoch: 43 | Loss: 2.4545 | Accuracy: 0.3012 | Val Loss: 0.2078 | Val Accuracy: 0.0232\n",
            "Time taken for epoch 43 44.28166484832764 sec\n",
            "\n",
            "Epoch: 44 | Loss: 2.4452 | Accuracy: 0.3021 | Val Loss: 0.2080 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 44 44.315794229507446 sec\n",
            "\n",
            "Epoch: 45 | Loss: 2.4350 | Accuracy: 0.3035 | Val Loss: 0.2083 | Val Accuracy: 0.0231\n",
            "Time taken for epoch 45 44.32491683959961 sec\n",
            "\n",
            "Epoch: 46 | Loss: 2.4234 | Accuracy: 0.3047 | Val Loss: 0.2086 | Val Accuracy: 0.0230\n",
            "Time taken for epoch 46 44.24301099777222 sec\n",
            "\n",
            "Epoch: 47 | Loss: 2.4126 | Accuracy: 0.3061 | Val Loss: 0.2093 | Val Accuracy: 0.0230\n",
            "Time taken for epoch 47 44.27734351158142 sec\n",
            "\n",
            "Epoch: 48 | Loss: 2.4001 | Accuracy: 0.3077 | Val Loss: 0.2096 | Val Accuracy: 0.0229\n",
            "Time taken for epoch 48 44.23386573791504 sec\n",
            "\n",
            "Epoch: 49 | Loss: 2.3831 | Accuracy: 0.3102 | Val Loss: 0.2102 | Val Accuracy: 0.0229\n",
            "Time taken for epoch 49 44.27455139160156 sec\n",
            "\n",
            "Epoch: 50 | Loss: 2.3680 | Accuracy: 0.3123 | Val Loss: 0.2115 | Val Accuracy: 0.0228\n",
            "Time taken for epoch 50 44.26340293884277 sec\n",
            "\n",
            "Epoch: 51 | Loss: 2.3523 | Accuracy: 0.3145 | Val Loss: 0.2124 | Val Accuracy: 0.0227\n",
            "Time taken for epoch 51 44.18422532081604 sec\n",
            "\n",
            "Epoch: 52 | Loss: 2.3320 | Accuracy: 0.3173 | Val Loss: 0.2133 | Val Accuracy: 0.0226\n",
            "Time taken for epoch 52 44.21482062339783 sec\n",
            "\n",
            "Epoch: 53 | Loss: 2.3110 | Accuracy: 0.3210 | Val Loss: 0.2147 | Val Accuracy: 0.0225\n",
            "Time taken for epoch 53 44.319374322891235 sec\n",
            "\n",
            "Epoch: 54 | Loss: 2.2869 | Accuracy: 0.3247 | Val Loss: 0.2160 | Val Accuracy: 0.0224\n",
            "Time taken for epoch 54 44.306551694869995 sec\n",
            "\n",
            "Epoch: 55 | Loss: 2.2620 | Accuracy: 0.3291 | Val Loss: 0.2174 | Val Accuracy: 0.0222\n",
            "Time taken for epoch 55 44.281630992889404 sec\n",
            "\n",
            "Epoch: 56 | Loss: 2.2344 | Accuracy: 0.3341 | Val Loss: 0.2194 | Val Accuracy: 0.0221\n",
            "Time taken for epoch 56 44.26360607147217 sec\n",
            "\n",
            "Epoch: 57 | Loss: 2.2080 | Accuracy: 0.3388 | Val Loss: 0.2216 | Val Accuracy: 0.0218\n",
            "Time taken for epoch 57 44.29673433303833 sec\n",
            "\n",
            "Epoch: 58 | Loss: 2.1760 | Accuracy: 0.3447 | Val Loss: 0.2229 | Val Accuracy: 0.0215\n",
            "Time taken for epoch 58 44.24348330497742 sec\n",
            "\n",
            "Epoch: 59 | Loss: 2.1418 | Accuracy: 0.3519 | Val Loss: 0.2258 | Val Accuracy: 0.0216\n",
            "Time taken for epoch 59 44.300713300704956 sec\n",
            "\n",
            "Epoch: 60 | Loss: 2.1079 | Accuracy: 0.3586 | Val Loss: 0.2280 | Val Accuracy: 0.0212\n",
            "Time taken for epoch 60 44.68671774864197 sec\n",
            "\n",
            "Epoch: 61 | Loss: 2.0705 | Accuracy: 0.3667 | Val Loss: 0.2303 | Val Accuracy: 0.0210\n",
            "Time taken for epoch 61 44.36995816230774 sec\n",
            "\n",
            "Epoch: 62 | Loss: 2.0336 | Accuracy: 0.3746 | Val Loss: 0.2330 | Val Accuracy: 0.0209\n",
            "Time taken for epoch 62 44.29550123214722 sec\n",
            "\n",
            "Epoch: 63 | Loss: 1.9940 | Accuracy: 0.3837 | Val Loss: 0.2366 | Val Accuracy: 0.0207\n",
            "Time taken for epoch 63 44.27580690383911 sec\n",
            "\n",
            "Epoch: 64 | Loss: 1.9525 | Accuracy: 0.3930 | Val Loss: 0.2395 | Val Accuracy: 0.0205\n",
            "Time taken for epoch 64 44.21112680435181 sec\n",
            "\n",
            "Epoch: 65 | Loss: 1.9093 | Accuracy: 0.4032 | Val Loss: 0.2434 | Val Accuracy: 0.0203\n",
            "Time taken for epoch 65 44.269471645355225 sec\n",
            "\n",
            "Epoch: 66 | Loss: 1.8659 | Accuracy: 0.4141 | Val Loss: 0.2467 | Val Accuracy: 0.0199\n",
            "Time taken for epoch 66 44.272459268569946 sec\n",
            "\n",
            "Epoch: 67 | Loss: 1.8189 | Accuracy: 0.4252 | Val Loss: 0.2497 | Val Accuracy: 0.0199\n",
            "Time taken for epoch 67 44.27907729148865 sec\n",
            "\n",
            "Epoch: 68 | Loss: 1.7759 | Accuracy: 0.4361 | Val Loss: 0.2545 | Val Accuracy: 0.0195\n",
            "Time taken for epoch 68 44.28588938713074 sec\n",
            "\n",
            "Epoch: 69 | Loss: 1.7282 | Accuracy: 0.4482 | Val Loss: 0.2579 | Val Accuracy: 0.0194\n",
            "Time taken for epoch 69 44.310330390930176 sec\n",
            "\n",
            "Epoch: 70 | Loss: 1.6819 | Accuracy: 0.4600 | Val Loss: 0.2628 | Val Accuracy: 0.0192\n",
            "Time taken for epoch 70 44.277348279953 sec\n",
            "\n",
            "Epoch: 71 | Loss: 1.6391 | Accuracy: 0.4715 | Val Loss: 0.2668 | Val Accuracy: 0.0193\n",
            "Time taken for epoch 71 44.244982957839966 sec\n",
            "\n",
            "Epoch: 72 | Loss: 1.5924 | Accuracy: 0.4841 | Val Loss: 0.2720 | Val Accuracy: 0.0189\n",
            "Time taken for epoch 72 44.253727197647095 sec\n",
            "\n",
            "Epoch: 73 | Loss: 1.5477 | Accuracy: 0.4959 | Val Loss: 0.2768 | Val Accuracy: 0.0188\n",
            "Time taken for epoch 73 44.36900591850281 sec\n",
            "\n",
            "Epoch: 74 | Loss: 1.5046 | Accuracy: 0.5076 | Val Loss: 0.2810 | Val Accuracy: 0.0185\n",
            "Time taken for epoch 74 44.29461312294006 sec\n",
            "\n",
            "Epoch: 75 | Loss: 1.4626 | Accuracy: 0.5192 | Val Loss: 0.2848 | Val Accuracy: 0.0185\n",
            "Time taken for epoch 75 44.34062123298645 sec\n",
            "\n",
            "Epoch: 76 | Loss: 1.4214 | Accuracy: 0.5307 | Val Loss: 0.2909 | Val Accuracy: 0.0183\n",
            "Time taken for epoch 76 44.30420207977295 sec\n",
            "\n",
            "Epoch: 77 | Loss: 1.3804 | Accuracy: 0.5423 | Val Loss: 0.2947 | Val Accuracy: 0.0179\n",
            "Time taken for epoch 77 44.28483605384827 sec\n",
            "\n",
            "Epoch: 78 | Loss: 1.3425 | Accuracy: 0.5530 | Val Loss: 0.2989 | Val Accuracy: 0.0180\n",
            "Time taken for epoch 78 44.43301272392273 sec\n",
            "\n",
            "Epoch: 79 | Loss: 1.3029 | Accuracy: 0.5636 | Val Loss: 0.3049 | Val Accuracy: 0.0178\n",
            "Time taken for epoch 79 44.17246055603027 sec\n",
            "\n",
            "Epoch: 80 | Loss: 1.2679 | Accuracy: 0.5739 | Val Loss: 0.3097 | Val Accuracy: 0.0178\n",
            "Time taken for epoch 80 44.72325873374939 sec\n",
            "\n",
            "Epoch: 81 | Loss: 1.2298 | Accuracy: 0.5849 | Val Loss: 0.3154 | Val Accuracy: 0.0175\n",
            "Time taken for epoch 81 44.22870755195618 sec\n",
            "\n",
            "Epoch: 82 | Loss: 1.1951 | Accuracy: 0.5945 | Val Loss: 0.3191 | Val Accuracy: 0.0175\n",
            "Time taken for epoch 82 44.43791103363037 sec\n",
            "\n",
            "Epoch: 83 | Loss: 1.1657 | Accuracy: 0.6032 | Val Loss: 0.3232 | Val Accuracy: 0.0175\n",
            "Time taken for epoch 83 44.366071939468384 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpT9D5_OgP6"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9BMm5vxovQR"
      },
      "source": [
        "#!rm -r /content/drive/MyDrive/training_checkpoints1024/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdxCA0F8pNow"
      },
      "source": [
        "source = \"/content/training_checkpoints/\"\r\n",
        "dest = \"/content/training_checkpoints{}Units{}Epochs{}Run/\".format(units, EPOCHS, run)\r\n",
        "os.rename(source, dest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxYDO-wrBUAa"
      },
      "source": [
        "f=open('History{}Units{}Epochs{}Run.txt'.format(units, EPOCHS, run),'w')\r\n",
        "# f.write('Total time:\\t' + str(total_time) + '\\t' + 'sec' + '\\n')\r\n",
        "f.write('TrainLoss\\t' + 'TrainAccuracy\\t' + 'ValLoss\\t' + 'ValAccuracy\\t' + 'Time\\n')\r\n",
        "for e in range(len(train_loss_vec)):\r\n",
        "    f.write(train_loss_vec[e]+'\\t'+ train_accuracy_vec[e]+ '\\t' + val_loss_vec[e]+ '\\t' + val_accuracy_vec[e]+ '\\t' + str(time_vec[e]) + '\\n')\r\n",
        "\r\n",
        "\r\n",
        "f.close()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GERgZiI7BJUm"
      },
      "source": [
        "history = [[],[],[],[], []]\r\n",
        "with open('History{}Units{}Epochs{}Run.txt'.format(units, EPOCHS, run)) as infile:\r\n",
        "  next(infile)\r\n",
        "  for line in infile:\r\n",
        "        history[0].append(float(line.split()[0]))\r\n",
        "        history[1].append(float(line.split()[1]))\r\n",
        "        history[2].append(float(line.split()[2]))\r\n",
        "        history[3].append(float(line.split()[3]))\r\n",
        "        history[4].append(float(line.split()[4]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idFzaOp0MaW9"
      },
      "source": [
        "#Model Loss\r\n",
        "fig = plt.figure(figsize=(9,5))\r\n",
        "ax = fig.add_subplot(1, 1, 1)\r\n",
        "line1 = ax.plot(history[0])\r\n",
        "line2 = ax.plot(history[2])\r\n",
        "ax.set_title('Model Loss')\r\n",
        "ax.set_ylabel('Loss')\r\n",
        "ax.set_xlabel('Epochs')\r\n",
        "ax.legend(('Train Loss', 'Val Loss'))\r\n",
        "plt.savefig('Model[Units{}Epochs{}Run{}]Loss'.format(units, EPOCHS, run))\r\n",
        "\r\n",
        "#Model Accuracy\r\n",
        "fig = plt.figure(figsize=(9,5))\r\n",
        "ax = fig.add_subplot(1, 1, 1)\r\n",
        "line1 = ax.plot(history[1])\r\n",
        "line2 = ax.plot(history[3])\r\n",
        "ax.set_title('Model Accuracy')\r\n",
        "ax.set_ylabel('Accuracy')\r\n",
        "ax.set_xlabel('Epochs')\r\n",
        "ax.legend(('Train Accuracy', 'Val Accuracy'))\r\n",
        "plt.savefig('Model[Units{}Epochs{}Run{}]Accuracy'.format(units, EPOCHS, run))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0dYLi0QgNLy"
      },
      "source": [
        "!cp -r training_checkpoints512Units100Epochs1Run/ '/content/512U100E1R'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfNTcmmXgiR-"
      },
      "source": [
        "!zip -r /content/256U500E1R.zip /content/256U500E1R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYo4JmSEpF5"
      },
      "source": [
        "# from google.colab import files\r\n",
        "# files.download('/content/256U50E2R.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pIZwv8ohuYz"
      },
      "source": [
        "!cp -r /content/256U500E1R.zip \"/content/drive/MyDrive/\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPCwPdUVSZ5"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhkcWcijMop5"
      },
      "source": [
        "def preprocess_sentence(w):\r\n",
        "        w = [w]\r\n",
        "        w_exp = remove_contractions(w)\r\n",
        "        w_punct = remove_punct(w_exp)\r\n",
        "        w_return = '<start> ' + w_punct[0] + ' <end>'\r\n",
        "        return w_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRaSOaKdGIGu"
      },
      "source": [
        "# Test Input\r\n",
        "g = \"! I'm, ... SURE.  you-? are23 ,CRAZY\"\r\n",
        "print(preprocess_sentence(g))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRtlN3aKHTSq"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\r\n",
        "  fig = plt.figure(figsize=(5,5))\r\n",
        "  ax = fig.add_subplot(1, 1, 1)\r\n",
        "  ax.matshow(attention, cmap='viridis')\r\n",
        "\r\n",
        "  fontdict = {'fontsize': 14}\r\n",
        "\r\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\r\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\r\n",
        "\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNuowc4dohEC"
      },
      "source": [
        "# sentiment = keras.models.load_model('/content/drive/MyDrive/sent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate_sentence(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)) for i in range(4)] # check\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
        "  end_token = targ_lang.word_index['<end>']\n",
        "\n",
        "  # greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "  greedy_sampler = tfa.seq2seq.SampleEmbeddingSampler() ###\n",
        "\n",
        "  # Instantiate BasicDecoder object\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "  # Setup Memory in decoder stack\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "  # set decoder_initial_state\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "\n",
        "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "  \n",
        "  outputs, _, _  = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
        "  attentions = decoder.get_attention_weights()\n",
        "  #sent_value = sentiment.predict(np.array([sentence]))\n",
        "  return outputs.sample_id.numpy(), attentions #, sent_value\n",
        "\n",
        "def translate(sentence):\n",
        "  result, attention_plot = evaluate_sentence(sentence) #da aggiungere sent_value\n",
        "  plot = [attention_plot[i].numpy() for i in range(len(attention_plot))]\n",
        "  # print(result)\n",
        "  result = targ_lang.sequences_to_texts(result)\n",
        "  result_cast = str(result[0]).split(' ')\n",
        "  sentence_cast = sentence.split(' ')\n",
        "  # print('User Input: %s' % (sentence))\n",
        "  print('Chatbot Output: {}'.format(result[0]))\n",
        "  # print(len(attention_plot))\n",
        "  len_res = len(result_cast)\n",
        "  len_sent = len(sentence_cast)\n",
        "  plt = plot\n",
        "  # plots = plot[0][:len_res, :len_sent] + plot[1][:len_res, :len_sent] + plot[2][:len_res, :len_sent] + plot[5][:len_res, :len_sent] + plot[6][:len_res, :len_sent]\n",
        "  plot = plot[0][:len_res, :len_sent]\n",
        "  plot_attention(plot, sentence_cast, result_cast)\n",
        "  #print('User Input is ', sent_value, ' positive')\n",
        "  print('----------------------------')\n",
        "  # return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI2cJ1JswJVB"
      },
      "source": [
        "#importare checkpoint motore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoYkwLWUpb_E"
      },
      "source": [
        "# sentiment.predict(np.array(['i think you are crazy']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZWTacz9Bcrj"
      },
      "source": [
        "\r\n",
        "class ChatBot:\r\n",
        "  # negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\r\n",
        "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\r\n",
        "#Method to start the conversation\r\n",
        "  def start_chat(self):\r\n",
        "    user_response = input(\"Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\\n\")\r\n",
        "    \r\n",
        "    # if user_response in self.negative_responses:\r\n",
        "    #   print(\"Ok, have a great day!\")\r\n",
        "    #   return\r\n",
        "    self.chat(user_response)\r\n",
        "#Method to handle the conversation\r\n",
        "  def chat(self, reply):\r\n",
        "    while not self.make_exit(reply):\r\n",
        "      reply = input(str(translate(reply))+\"\\n\")\r\n",
        "    \r\n",
        "\r\n",
        "  def make_exit(self, reply):\r\n",
        "    for exit_command in self.exit_commands:\r\n",
        "      if exit_command in reply:\r\n",
        "        print(\"Ok, have a great day!\")\r\n",
        "        return True\r\n",
        "    return False\r\n",
        "  \r\n",
        "chatbot = ChatBot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRLj8vpWASyo"
      },
      "source": [
        "chatbot.start_chat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu9ZuEyHwH0F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}